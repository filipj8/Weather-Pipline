{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is the number the step size is divided by (aka larger d yields smaller bin size)\n",
    "# Step sizes used: [400, 200, 100, 50, 25, 18, 12.5]\n",
    "d = 18\n",
    "\n",
    "destdir = 'BinnedCsvs_d{}'.format(d)\n",
    "csvs = [ os.path.join(destdir,f) for f in os.listdir(destdir) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main_check(csv, reduced=False):\n",
    "    '''Check csv, if reduced == True, check csv when it has been reduced to the top 10 stations'''\n",
    "    try:\n",
    "        csv_passes = True\n",
    "\n",
    "        # Read in the csv (Created by GHCN_Binned.ipynb)\n",
    "        df = pd.read_csv(csv, index_col=0)\n",
    "        # Only want TMAX and TMIN elements\n",
    "        # df = df[df.element.isin(['TMAX', 'TMIN'])]\n",
    "\n",
    "        # New binning used different variable names, for compatability\n",
    "        df.reset_index(0, inplace=True)\n",
    "        df.columns = ['id', 'date', 'element', 'value']\n",
    "\n",
    "        # Use datetime for easy indexing\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        \n",
    "        \n",
    "        ################### REDUCE #########################\n",
    "        # If we want to perform the checks using only the top 10 most frequent stations in each bin\n",
    "        if reduced == True:\n",
    "            station_count_df = df.groupby('id')['id'].agg({'station_count':'count'})\n",
    "            station_list_sorted = list(station_count_df.sort_values('station_count', ascending=False).index)\n",
    "            top10_stations = station_list_sorted[:10]\n",
    "\n",
    "            df = df[df['id'].isin(top10_stations)]\n",
    "        ####################################################\n",
    "\n",
    "        # Convert table from long format to wide\n",
    "        df_wide = df.pivot_table(index=['id', 'date'], columns=['element'], values='value')\n",
    "\n",
    "        # Convert temperature readings to degrees C and precipitation readings to mm \n",
    "        # (not needed for checking)\n",
    "        # df_wide['TMAX'] /= 10\n",
    "        # df_wide['TMIN'] /= 10\n",
    "\n",
    "        df_wide.reset_index(0, inplace=True)\n",
    "\n",
    "        # Check 1: make sure the datafile has data for years 2005-2015 and correct columns\n",
    "        # Returns True if the data passes the check\n",
    "        def check_1():\n",
    "            a = set(df_wide.columns) >= {'TMAX', 'TMIN', 'id'}\n",
    "            b = set(np.arange(2005,2016)) <= set(df_wide.index.year)\n",
    "            return a and b\n",
    "        \n",
    "        # Update csv_passes to reflect result from the first check\n",
    "        csv_passes = check_1()\n",
    "    \n",
    "        # If the first check is failed, return\n",
    "        if csv_passes == False:\n",
    "            return (csv, csv_passes)\n",
    "    \n",
    "        #########################################################################\n",
    "\n",
    "        # Number of years to aggregate over that will be compared against final_year\n",
    "        num_years = 10\n",
    "\n",
    "        # The final year that will be used to check for broken records\n",
    "        final_year = 2015\n",
    "\n",
    "        #########################################################################\n",
    "\n",
    "        # Split the data into the last year and the previous years\n",
    "        previousyears = df_wide[str(final_year-num_years):str(final_year-1)]\n",
    "        lastyear = df_wide[str(final_year)]\n",
    "\n",
    "        # Calculate previous years' record highs and lows\n",
    "        previousyears_maxmin = pd.groupby(previousyears, by=[previousyears.index.month,previousyears.index.day]).agg({'TMAX':'max', 'TMIN':'min'})\n",
    "\n",
    "        # Calculate the last year's record highs and lows\n",
    "        lastyear_maxmin = pd.groupby(lastyear, by=[lastyear.index.month,lastyear.index.day]).agg({'TMAX':'max', 'TMIN':'min'})\n",
    "\n",
    "\n",
    "        # Only do check 2 if check 1 passes\n",
    "        # check 2: length of lastyear_maxmin['TMAX'] lastyear_maxmin['TMIN'] previousyears_maxmin['TMAX'] \n",
    "        # and previousyears_maxmin['TMIN'] should be 365 or 366.\n",
    "        # Returns True if the data passes the check\n",
    "        def check_2():\n",
    "            # Get lengths\n",
    "            n1 = lastyear_maxmin['TMAX'].count()\n",
    "            n2 = lastyear_maxmin['TMIN'].count()\n",
    "            n3 = previousyears_maxmin['TMAX'].count()\n",
    "            n4 = previousyears_maxmin['TMIN'].count()\n",
    "            # Check that lengths are 365 or 366\n",
    "            return set([n1, n2, n3, n4]) <= set([365, 366])\n",
    "\n",
    "\n",
    "        # Only do check 3 if check 2 passes\n",
    "        # check 3: There are at least x tmin outliers + tmax outliers in the last year\n",
    "        # Returns True if the data passes the check\n",
    "        def check_3(x=1):\n",
    "            # Deal with leap years\n",
    "            leap_day_missing = False\n",
    "\n",
    "            try:\n",
    "                leap_day_missing = False if previousyears_maxmin.xs([2,29]).count() == 2 else True\n",
    "            except:\n",
    "                leap_day_missing = True\n",
    "\n",
    "            if not leap_day_missing:\n",
    "                previousyears_WLY = previousyears_maxmin.loc[(previousyears_maxmin.index.get_level_values(0) != 2) | \n",
    "                                                             (previousyears_maxmin.index.get_level_values(1) != 29)]\n",
    "            else:\n",
    "                previousyears_WLY = previousyears_maxmin\n",
    "\n",
    "            # Find record highs and lows (outliers)\n",
    "            record_high_lastyear = lastyear_maxmin['TMAX'].where(lastyear_maxmin['TMAX'] >= previousyears_WLY['TMAX'])\n",
    "            record_low_lastyear = lastyear_maxmin['TMIN'].where(lastyear_maxmin['TMIN'] <= previousyears_WLY['TMIN'])\n",
    "\n",
    "            total_number_of_outliers = record_high_lastyear.count()+record_low_lastyear.count()\n",
    "\n",
    "            return total_number_of_outliers >= x\n",
    "\n",
    "\n",
    "        csv_passes = check_2()\n",
    "\n",
    "        if csv_passes == False:\n",
    "            return (csv, csv_passes)\n",
    "        else:\n",
    "            csv_passes = check_3()\n",
    "            return (csv, csv_passes)\n",
    "    except:\n",
    "        return '{} failed!'.format(csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 104/104 [04:32<00:00,  5.91s/it]\n"
     ]
    }
   ],
   "source": [
    "# Iterate through all the csvs and test them using main_check\n",
    "results = []\n",
    "for csv in tqdm(csvs):\n",
    "    results.append(main_check(csv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is/are 0 csv/s that produced an error.\n"
     ]
    }
   ],
   "source": [
    "# These are the csvs that raised an error\n",
    "error_csvs = []\n",
    "for i in results:\n",
    "    if type(i) != tuple:\n",
    "        csv_name = i.split()[0]\n",
    "        error_csvs.append(csv_name)\n",
    "print('There is/are {} csv/s that produced an error.'.format(len(error_csvs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 46 csvs that failed.\n"
     ]
    }
   ],
   "source": [
    "# These are the csvs that failed (should be deleted)\n",
    "failed_csvs = []\n",
    "for i in results:\n",
    "    if i[1] == False:\n",
    "        failed_csvs.append(i[0])\n",
    "print('There are {} csvs that failed.'.format(len(failed_csvs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 58 csvs that passed.\n"
     ]
    }
   ],
   "source": [
    "# These are the csvs that passed (should be used for the assignment)\n",
    "passed_csvs = []\n",
    "for i in results:\n",
    "    if i[1] == True:\n",
    "        passed_csvs.append(i[0])\n",
    "print('There are {} csvs that passed.'.format(len(passed_csvs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check\n",
    "len(failed_csvs) + len(passed_csvs) + len(error_csvs) == len(csvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check what stations are in the passed_csvs\n",
    "passed_stations = []\n",
    "for csv in passed_csvs:\n",
    "    passed_df = pd.read_csv(csv, index_col=0)\n",
    "    passed_stations += list(passed_df.index.unique())\n",
    "\n",
    "print('There are {} stations left!'.format(len(passed_stations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save the stations that passed to csv\n",
    "#pd.Series(passed_stations).to_csv('passed_stations_d{}.csv'.format(d))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
