{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather Data Pipeline for [C2A2](C2A2.ipynb)\n",
    "\n",
    "[GHCN_Daily Readme](ghcn_daily_readme.txt)\n",
    "\n",
    "[GHCN_Daily by year Readme](ghcn_daily_by_year_readme.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [GHCN-D Downloader](GHCN-D Downloader.ipynb)\n",
    "\n",
    "Downloads the GHCN-D data and cleans it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# These are the years we want to get the data for\n",
    "years = np.arange(11)+2005\n",
    "\n",
    "ghcnd = []\n",
    "\n",
    "# For each year, download the corresponding data, and perform cleaning\n",
    "for year in tqdm(years):\n",
    "    df = pd.read_csv('ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/by_year/{}.csv.gz'.format(year),\n",
    "                        compression='gzip', \n",
    "                        index_col=0, \n",
    "                        header=None,\n",
    "                        usecols=[0,1,2,3,5],\n",
    "                        names=['ID', 'Date', 'Element', 'Data_Value', 'M-FLAG', 'Q-FLAG', 'S-FLAG', 'OBS-TIME'],\n",
    "                        parse_dates = [1],\n",
    "                        infer_datetime_format=True)\n",
    "    df = df[(df['Element'].isin(['TMAX', 'TMIN'])) & (df['Q-FLAG'].isnull()) & (df['Data_Value'].notnull())]\n",
    "    df = df.drop('Q-FLAG', axis=1)\n",
    "    ghcnd.append(df)\n",
    "\n",
    "# Create master DataFrame\n",
    "ghcnd_df = pd.concat(ghcnd)\n",
    "\n",
    "# Save to hdf5\n",
    "ghcnd_df.to_hdf('GHCND_10Year.h5', 'ghcn_df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Binning](Binning.ipynb)\n",
    "\n",
    "Splits the surface of the earth into equal area bins. Creates folders by bin size and in each folder are csvs of the binned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hashlib\n",
    "from tqdm import tqdm\n",
    "\n",
    "def reproject(latitude, longitude):\n",
    "        \"\"\"Returns the x & y coordinates in meters using a sinusoidal projection\"\"\"\n",
    "        earth_radius = 6371009 # in meters\n",
    "        lat_dist = np.pi * earth_radius / 180.0\n",
    "\n",
    "        y = latitude*lat_dist\n",
    "        x = longitude * lat_dist * np.cos(np.radians(latitude))\n",
    "        return x, y\n",
    "\n",
    "    \n",
    "def hash_it(s):\n",
    "    return hashlib.sha224(s.encode('utf-8')).hexdigest()    \n",
    "\n",
    "\n",
    "def Bin_and_Split(d):\n",
    "\n",
    "    stations = pd.read_csv('ghcnd-stations.csv', index_col=0)\n",
    "    \n",
    "    # Reduce the stations to the ones that exist in ghcn_df (from GHCN-D Downloader.ipynb)\n",
    "    stations = stations[stations.index.isin(stations_with_data)]\n",
    "\n",
    "    stations['x'], stations['y'] = reproject(stations['LATITUDE'], stations['LONGITUDE'])\n",
    "\n",
    "    step_size = (stations['x'].max()-stations['x'].min())/d\n",
    "\n",
    "    # Bin stations \n",
    "    labels = [ \"{0} to {1}\".format(i, i + step_size) for i in np.arange(stations['x'].min(), stations['x'].max(), step_size) ]\n",
    "    stations['x_group'] = pd.cut(stations.x, np.arange(stations['x'].min(), stations['x'].max()+step_size, step_size), right=False, labels=labels)\n",
    "    stations['y_group'] = pd.cut(stations.y, np.arange(stations['x'].min(), stations['x'].max()+step_size, step_size), right=False, labels=labels)\n",
    "    stations['xy_group'] = stations['x_group'].astype('str') + ', ' + stations['y_group'].astype('str')\n",
    "    \n",
    "    # Find hashing from xy_group\n",
    "    stations['hash']=stations['xy_group'].apply(hash_it)\n",
    "    \n",
    "    # Map hashing to ghcn data, sort for faster querying\n",
    "    ghcn2 = ghcn_df.copy()\n",
    "    ghcn2['hash'] = stations['hash']\n",
    "    ghcn2 = ghcn2.sort_values('hash')\n",
    "    ghcn3 = ghcn2.drop('hash', axis=1)\n",
    "\n",
    "    # Create the folder that will hold the csvs for that specific step size\n",
    "    new_folder = 'BinnedCsvs_d{}'.format(d)\n",
    "    os.mkdir(new_folder)\n",
    "    \n",
    "    # For each hash, select from ghcn data and save to csv\n",
    "    sorted_hashes = np.sort(stations['hash'].unique())\n",
    "    for hashid in sorted_hashes:\n",
    "        left, = ghcn2['hash'].searchsorted(hashid, 'left')\n",
    "        right, = ghcn2['hash'].searchsorted(hashid, 'right')\n",
    "        df_by_bin = ghcn3.iloc[left:right]\n",
    "        df_by_bin.to_csv('./'+new_folder+'/{}.csv'.format(hashid))\n",
    "\n",
    "    stations.to_csv('BinSize_d{}.csv'.format(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are the step sizes we want to use\n",
    "step_sizes = [400, 200, 100, 50, 25, 18, 12.5]\n",
    "\n",
    "# Load ghcn data\n",
    "ghcn_df = pd.read_hdf('GHCND_10Year.h5')\n",
    "\n",
    "# Pull out the set of stations that have passed the initial requirements from `GHCN-D Downloader`\n",
    "stations_with_data = set(ghcn_df.index)\n",
    "\n",
    "for s in tqdm(step_sizes):\n",
    "    Bin_and_Split(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [CSV_Checker](CSV_Checker.ipynb)\n",
    "\n",
    "Checks each csv for three conditions:\n",
    "\n",
    "* make sure the datafile has data for years 2005-2015 and correct columns\n",
    "* length of `lastyear_maxmin['TMAX']`, `lastyear_maxmin['TMIN']`, `previousyears_maxmin['TMAX']`, and `previousyears_maxmin['TMIN']` should be 365 or 366.\n",
    "* There are at least `x` `tmin` outliers + `tmax` outliers in the last year (`x` default is 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# This is the number the step size is divided by (aka larger d yields smaller bin size)\n",
    "# Step sizes used: [400, 200, 100, 50, 25, 18, 12.5]\n",
    "d = 18\n",
    "\n",
    "# Get a list of the csvs in the directory\n",
    "destdir = 'BinnedCsvs_d{}'.format(d)\n",
    "csvs = [ os.path.join(destdir,f) for f in os.listdir(destdir) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main_check(csv, reduced=False):\n",
    "    '''Check csv, if reduced == True, check csv when it has been reduced to the top 10 stations'''\n",
    "    try:\n",
    "        csv_passes = True\n",
    "\n",
    "        # Read in the csv (Created by GHCN_Binned.ipynb)\n",
    "        df = pd.read_csv(csv, index_col=0)\n",
    "        # Only want TMAX and TMIN elements\n",
    "        # df = df[df.element.isin(['TMAX', 'TMIN'])]\n",
    "\n",
    "        # New binning used different variable names, for compatability\n",
    "        df.reset_index(0, inplace=True)\n",
    "        df.columns = ['id', 'date', 'element', 'value']\n",
    "\n",
    "        # Use datetime for easy indexing\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        \n",
    "        \n",
    "        ################### REDUCE #########################\n",
    "        # If we want to perform the checks using only the top 10 most frequent stations in each bin\n",
    "        if reduced == True:\n",
    "            station_count_df = df.groupby('id')['id'].agg({'station_count':'count'})\n",
    "            station_list_sorted = list(station_count_df.sort_values('station_count', ascending=False).index)\n",
    "            top10_stations = station_list_sorted[:10]\n",
    "\n",
    "            df = df[df['id'].isin(top10_stations)]\n",
    "        ####################################################\n",
    "\n",
    "        # Convert table from long format to wide\n",
    "        df_wide = df.pivot_table(index=['id', 'date'], columns=['element'], values='value')\n",
    "\n",
    "        # Convert temperature readings to degrees C and precipitation readings to mm \n",
    "        # (not needed for checking)\n",
    "        # df_wide['TMAX'] /= 10\n",
    "        # df_wide['TMIN'] /= 10\n",
    "\n",
    "        df_wide.reset_index(0, inplace=True)\n",
    "\n",
    "        # Check 1: make sure the datafile has data for years 2005-2015 and correct columns\n",
    "        # Returns True if the data passes the check\n",
    "        def check_1():\n",
    "            a = set(df_wide.columns) >= {'TMAX', 'TMIN', 'id'}\n",
    "            b = set(np.arange(2005,2016)) <= set(df_wide.index.year)\n",
    "            return a and b\n",
    "        \n",
    "        # Update csv_passes to reflect result from the first check\n",
    "        csv_passes = check_1()\n",
    "    \n",
    "        # If the first check is failed, return\n",
    "        if csv_passes == False:\n",
    "            return (csv, csv_passes)\n",
    "    \n",
    "        #########################################################################\n",
    "\n",
    "        # Number of years to aggregate over that will be compared against final_year\n",
    "        num_years = 10\n",
    "\n",
    "        # The final year that will be used to check for broken records\n",
    "        final_year = 2015\n",
    "\n",
    "        #########################################################################\n",
    "\n",
    "        # Split the data into the last year and the previous years\n",
    "        previousyears = df_wide[str(final_year-num_years):str(final_year-1)]\n",
    "        lastyear = df_wide[str(final_year)]\n",
    "\n",
    "        # Calculate previous years' record highs and lows\n",
    "        previousyears_maxmin = pd.groupby(previousyears, by=[previousyears.index.month,previousyears.index.day]).agg({'TMAX':'max', 'TMIN':'min'})\n",
    "\n",
    "        # Calculate the last year's record highs and lows\n",
    "        lastyear_maxmin = pd.groupby(lastyear, by=[lastyear.index.month,lastyear.index.day]).agg({'TMAX':'max', 'TMIN':'min'})\n",
    "\n",
    "\n",
    "        # Only do check 2 if check 1 passes\n",
    "        # check 2: length of lastyear_maxmin['TMAX'] lastyear_maxmin['TMIN'] previousyears_maxmin['TMAX'] \n",
    "        # and previousyears_maxmin['TMIN'] should be 365 or 366.\n",
    "        # Returns True if the data passes the check\n",
    "        def check_2():\n",
    "            # Get lengths\n",
    "            n1 = lastyear_maxmin['TMAX'].count()\n",
    "            n2 = lastyear_maxmin['TMIN'].count()\n",
    "            n3 = previousyears_maxmin['TMAX'].count()\n",
    "            n4 = previousyears_maxmin['TMIN'].count()\n",
    "            # Check that lengths are 365 or 366\n",
    "            return set([n1, n2, n3, n4]) <= set([365, 366])\n",
    "\n",
    "\n",
    "        # Only do check 3 if check 2 passes\n",
    "        # check 3: There are at least x tmin outliers + tmax outliers in the last year\n",
    "        # Returns True if the data passes the check\n",
    "        def check_3(x=1):\n",
    "            # Deal with leap years\n",
    "            leap_day_missing = False\n",
    "\n",
    "            try:\n",
    "                leap_day_missing = False if previousyears_maxmin.xs([2,29]).count() == 2 else True\n",
    "            except:\n",
    "                leap_day_missing = True\n",
    "\n",
    "            if not leap_day_missing:\n",
    "                previousyears_WLY = previousyears_maxmin.loc[(previousyears_maxmin.index.get_level_values(0) != 2) | \n",
    "                                                             (previousyears_maxmin.index.get_level_values(1) != 29)]\n",
    "            else:\n",
    "                previousyears_WLY = previousyears_maxmin\n",
    "\n",
    "            # Find record highs and lows (outliers)\n",
    "            record_high_lastyear = lastyear_maxmin['TMAX'].where(lastyear_maxmin['TMAX'] >= previousyears_WLY['TMAX'])\n",
    "            record_low_lastyear = lastyear_maxmin['TMIN'].where(lastyear_maxmin['TMIN'] <= previousyears_WLY['TMIN'])\n",
    "\n",
    "            total_number_of_outliers = record_high_lastyear.count()+record_low_lastyear.count()\n",
    "\n",
    "            return total_number_of_outliers >= x\n",
    "\n",
    "\n",
    "        csv_passes = check_2()\n",
    "\n",
    "        if csv_passes == False:\n",
    "            return (csv, csv_passes)\n",
    "        else:\n",
    "            csv_passes = check_3()\n",
    "            return (csv, csv_passes)\n",
    "    except:\n",
    "        return '{} failed!'.format(csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Iterate through all the csvs and test them using main_check\n",
    "results = []\n",
    "for csv in tqdm(csvs):\n",
    "    results.append(main_check(csv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are the csvs that raised an error\n",
    "error_csvs = []\n",
    "for i in results:\n",
    "    if type(i) != tuple:\n",
    "        csv_name = i.split()[0]\n",
    "        error_csvs.append(csv_name)\n",
    "print('There is/are {} csv/s that produced an error.'.format(len(error_csvs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are the csvs that failed (should be deleted)\n",
    "failed_csvs = []\n",
    "for i in results:\n",
    "    if i[1] == False:\n",
    "        failed_csvs.append(i[0])\n",
    "print('There are {} csvs that failed.'.format(len(failed_csvs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are the csvs that passed (should be used for the assignment)\n",
    "passed_csvs = []\n",
    "for i in results:\n",
    "    if i[1] == True:\n",
    "        passed_csvs.append(i[0])\n",
    "print('There are {} csvs that passed.'.format(len(passed_csvs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "len(failed_csvs) + len(passed_csvs) + len(error_csvs) == len(csvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check what stations are in the passed_csvs\n",
    "passed_stations = []\n",
    "for csv in passed_csvs:\n",
    "    passed_df = pd.read_csv(csv, index_col=0)\n",
    "    passed_stations += list(passed_df.index.unique())\n",
    "\n",
    "print('There are {} stations left!'.format(len(passed_stations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the stations that passed to csv\n",
    "#pd.Series(passed_stations).to_csv('passed_stations_d{}.csv'.format(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Plotting](Plot_Bin.ipynb)\n",
    "\n",
    "Plotting functions to show the learner where the data is coming from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Stations (Basemap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.basemap import Basemap\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def plot_stations(binsize, hashid):\n",
    "\n",
    "    df = pd.read_csv('BinSize_d{}.csv'.format(binsize))\n",
    "\n",
    "    station_locations_by_hash = df[df['hash'] == hashid]\n",
    "\n",
    "    lons = station_locations_by_hash['LONGITUDE'].tolist()\n",
    "    lats = station_locations_by_hash['LATITUDE'].tolist()\n",
    "\n",
    "    mean_lat = np.mean(lats)\n",
    "    mean_lon = np.mean(lons)\n",
    "    max_lat = np.max(lats)\n",
    "    min_lat = np.min(lats)\n",
    "    max_lon = np.max(lons)\n",
    "    min_lon = np.min(lons)\n",
    "    buffer = 3\n",
    "\n",
    "    map = Basemap(projection='merc', resolution='l', \n",
    "                  lat_0=mean_lat, lon_0=mean_lon,\n",
    "                  llcrnrlon=min_lon-buffer, llcrnrlat=min_lat-buffer,\n",
    "                  urcrnrlon=max_lon+buffer, urcrnrlat=max_lat+buffer)\n",
    "\n",
    "    map.drawmapboundary(fill_color='darkblue')\n",
    "    map.fillcontinents(color='white',lake_color='darkblue', zorder=0)\n",
    "    map.drawcoastlines()\n",
    "\n",
    "    x, y = map(lons,lats)\n",
    "\n",
    "    map.scatter(x, y, marker='o',color='r', alpha=0.7, zorder=10)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_stations(50, '7a12fe430a08c938daaaeab98da6a60744de60df8d6159d82fe3a764')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leaflet\n",
    "\n",
    "#### Plot Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mplleaflet\n",
    "import pandas as pd\n",
    "\n",
    "def leaflet_plot_stations(binsize, hashid):\n",
    "\n",
    "    df = pd.read_csv('BinSize_d{}.csv'.format(binsize))\n",
    "\n",
    "    station_locations_by_hash = df[df['hash'] == hashid]\n",
    "\n",
    "    lons = station_locations_by_hash['LONGITUDE'].tolist()\n",
    "    lats = station_locations_by_hash['LATITUDE'].tolist()\n",
    "\n",
    "    plt.figure(figsize=(8,8))\n",
    "\n",
    "    plt.scatter(lons, lats, c='r', alpha=0.7, s=200)\n",
    "\n",
    "    return mplleaflet.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "leaflet_plot_stations(400, 'ba06c674e1bbe085f70f2cde04a98e78f46a1d7ae56ebdcd2a61c6ff')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Hull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial import ConvexHull\n",
    "import matplotlib.pyplot as plt\n",
    "import mplleaflet\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def leaflet_plot_hull(binsize, hashid):\n",
    "    \n",
    "    df = pd.read_csv('BinSize_d{}.csv'.format(binsize))\n",
    "\n",
    "    station_locations_by_hash = df[df['hash'] == hashid]\n",
    "\n",
    "    lons = station_locations_by_hash['LONGITUDE'].tolist()\n",
    "    lats = station_locations_by_hash['LATITUDE'].tolist()\n",
    "\n",
    "    points = np.array([lons,lats]).T\n",
    "    hull = ConvexHull(points)\n",
    "\n",
    "    plt.figure(figsize=(8,8))\n",
    "\n",
    "    plt.fill(points[hull.vertices,0], points[hull.vertices,1], 'k', alpha=0.3)\n",
    "\n",
    "    for simplex in hull.simplices:\n",
    "        plt.plot(points[simplex, 0], points[simplex, 1], 'k-', lw=5)\n",
    "\n",
    "    return mplleaflet.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "leaflet_plot_hull(400, 'ba06c674e1bbe085f70f2cde04a98e78f46a1d7ae56ebdcd2a61c6ff')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ~Additional~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def reproject(latitude, longitude):\n",
    "    \"\"\"Returns the x & y coordinates in meters using a sinusoidal projection\"\"\"\n",
    "    earth_radius = 6371009 # in meters\n",
    "    lat_dist = np.pi * earth_radius / 180.0\n",
    "\n",
    "    y = latitude*lat_dist\n",
    "    x = longitude * lat_dist * np.cos(np.radians(latitude))\n",
    "    return x, y\n",
    "\n",
    "def deproject(x,y):\n",
    "    '''Returns the latitude and longitude coordinates from x and y'''\n",
    "    earth_radius = 6371009 # in meters\n",
    "    lat_dist = np.pi * earth_radius / 180.0\n",
    "\n",
    "    latitude = y / lat_dist\n",
    "    longitude = x / (np.cos(np.radians(latitude))* lat_dist)\n",
    "    return latitude, longitude\n",
    "\n",
    "def latlon_from_xygroup(xygroup):\n",
    "    '''Returns corner 1 latitude and longitude, and corner 2 latitude and longitude.''' \n",
    "    '''xygroup looks like '-6571743.681047112 to -6474410.252330746, 1896264.6172766648 to 1993598.0459930308'''\n",
    "    left_x = float(xygroup.split()[0])\n",
    "    right_x = float(xygroup.split()[2][:-1])\n",
    "    left_y = float(xygroup.split()[3])\n",
    "    right_y = float(xygroup.split()[5])\n",
    "    c1_lat, c1_lon = deproject(left_x, left_y)\n",
    "    c2_lat, c2_lon = deproject(right_x, right_y)\n",
    "    return c1_lat, c1_lon, c2_lat, c2_lon"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
